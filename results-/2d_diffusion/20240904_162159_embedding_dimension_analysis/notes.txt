# Title: Optimizing Embedding Dimensions in Low-Dimensional Diffusion Models
# Experiment description: 1. Modify MLPDenoiser to accept embedding_dim as a parameter for both time and input embeddings. 2. Categorize datasets by complexity (e.g., number of modes: circle=1, line=1, moons=2, dino=multiple). 3. Train models with various embedding dimensions (32, 64, 128, 256, 512) on each dataset. 4. Compare performance using final KL divergence, steps to reach KL threshold, visual sample quality, and training time. 5. Analyze the relationship between dataset complexity and optimal embedding dimension. 6. Investigate potential overfitting and diminishing returns with very high embedding dimensions.
## Run 0: Baseline
Results: {'circle': {'training_time': 22.78913640975952, 'eval_loss': 0.4352689213155176, 'inference_time': 0.16325092315673828, 'kl_divergence': 0.3390841615593227}, 'dino': {'training_time': 21.349066257476807, 'eval_loss': 0.6604227140126631, 'inference_time': 0.15765833854675293, 'kl_divergence': 1.0292828033411612}, 'line': {'training_time': 20.768935203552246, 'eval_loss': 0.8046935080262401, 'inference_time': 0.15740132331848145, 'kl_divergence': 0.1572009839576215}, 'moons': {'training_time': 20.56239104270935, 'eval_loss': 0.614176513593825, 'inference_time': 0.12911629676818848, 'kl_divergence': 0.09562104554894295}}
Description: Baseline results.
